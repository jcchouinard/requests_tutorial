{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Requests\n",
    "\n",
    "### by: Jean-Christophe Chouinard\n",
    "\n",
    "### Twitter: @ChouinardJC\n",
    "\n",
    "### Blog: jcchouinard.com\n",
    "\n",
    "### Link to article: jcchouinard.com/python-requests/\n",
    "\n",
    "---------\n",
    "\n",
    "## Install Package\n",
    "\n",
    "jcchouinard.com/install-python-with-anaconda-on-windows/\n",
    "\n",
    "`$ pip install requests`\n",
    "\n",
    "`$ pip install beautifulsoup4`\n",
    "\n",
    "`$ pip install urllib`\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests basics\n",
    "\n",
    "- GET: get content of a page \n",
    "- POST: post a status on social media\n",
    "- Other methods not covered here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple GET request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL:  https://crawler-test.com/\n",
      "Status code:  200\n",
      "HTTP header:  {'Content-Encoding': 'gzip', 'Content-Type': 'text/html;charset=utf-8', 'Date': 'Wed, 06 Oct 2021 00:11:45 GMT', 'Server': 'nginx/1.10.3', 'Vary': 'Accept-Encoding', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-XSS-Protection': '1; mode=block', 'Content-Length': '8098', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "url = 'https://crawler-test.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "print('URL: ',  response.url)\n",
    "print('Status code: ', response.status_code)\n",
    "print('HTTP header: ', response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple POST request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {'last_name': 'Chouinard',\n",
       "  'name': 'Jean-Christophe',\n",
       "  'website': 'https://www.jcchouinard.com/'},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate',\n",
       "  'Content-Length': '85',\n",
       "  'Content-Type': 'application/x-www-form-urlencoded',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.24.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-615ce9f8-5b5ab52047b4e17c398995b2'},\n",
       " 'json': None,\n",
       " 'origin': '149.167.130.162',\n",
       " 'url': 'https://httpbin.org/post'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "url = 'https://httpbin.org/post'\n",
    "payload = {\n",
    "    'name':'Jean-Christophe',\n",
    "    'last_name':'Chouinard',\n",
    "    'website':'https://www.jcchouinard.com/'\n",
    "    }\n",
    "r = requests.post(url, data=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Methods and Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "url = 'https://crawler-test.com/'\n",
    "r = requests.get(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Response in module requests.models object:\n",
      "\n",
      "class Response(builtins.object)\n",
      " |  The :class:`Response <Response>` object, which contains a\n",
      " |  server's response to an HTTP request.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |      Returns True if :attr:`status_code` is less than 400.\n",
      " |      \n",
      " |      This attribute checks if the status code of the response is between\n",
      " |      400 and 600 to see if there was a client error or a server error. If\n",
      " |      the status code, is between 200 and 400, this will return True. This\n",
      " |      is **not** a check to see if the response code is ``200 OK``.\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Allows you to use a response as an iterator.\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |      Returns True if :attr:`status_code` is less than 400.\n",
      " |      \n",
      " |      This attribute checks if the status code of the response is between\n",
      " |      400 and 600 to see if there was a client error or a server error. If\n",
      " |      the status code, is between 200 and 400, this will return True. This\n",
      " |      is **not** a check to see if the response code is ``200 OK``.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  close(self)\n",
      " |      Releases the connection back to the pool. Once this method has been\n",
      " |      called the underlying ``raw`` object must not be accessed again.\n",
      " |      \n",
      " |      *Note: Should not normally need to be called explicitly.*\n",
      " |  \n",
      " |  iter_content(self, chunk_size=1, decode_unicode=False)\n",
      " |      Iterates over the response data.  When stream=True is set on the\n",
      " |      request, this avoids reading the content at once into memory for\n",
      " |      large responses.  The chunk size is the number of bytes it should\n",
      " |      read into memory.  This is not necessarily the length of each item\n",
      " |      returned as decoding can take place.\n",
      " |      \n",
      " |      chunk_size must be of type int or None. A value of None will\n",
      " |      function differently depending on the value of `stream`.\n",
      " |      stream=True will read data as it arrives in whatever size the\n",
      " |      chunks are received. If stream=False, data is returned as\n",
      " |      a single chunk.\n",
      " |      \n",
      " |      If decode_unicode is True, content will be decoded using the best\n",
      " |      available encoding based on the response.\n",
      " |  \n",
      " |  iter_lines(self, chunk_size=512, decode_unicode=False, delimiter=None)\n",
      " |      Iterates over the response data, one line at a time.  When\n",
      " |      stream=True is set on the request, this avoids reading the\n",
      " |      content at once into memory for large responses.\n",
      " |      \n",
      " |      .. note:: This method is not reentrant safe.\n",
      " |  \n",
      " |  json(self, **kwargs)\n",
      " |      Returns the json-encoded content of a response, if any.\n",
      " |      \n",
      " |      :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n",
      " |      :raises ValueError: If the response body does not contain valid json.\n",
      " |  \n",
      " |  raise_for_status(self)\n",
      " |      Raises :class:`HTTPError`, if one occurred.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  apparent_encoding\n",
      " |      The apparent encoding, provided by the chardet library.\n",
      " |  \n",
      " |  content\n",
      " |      Content of the response, in bytes.\n",
      " |  \n",
      " |  is_permanent_redirect\n",
      " |      True if this Response one of the permanent versions of redirect.\n",
      " |  \n",
      " |  is_redirect\n",
      " |      True if this Response is a well-formed HTTP redirect that could have\n",
      " |      been processed automatically (by :meth:`Session.resolve_redirects`).\n",
      " |  \n",
      " |  links\n",
      " |      Returns the parsed header links of the response, if any.\n",
      " |  \n",
      " |  next\n",
      " |      Returns a PreparedRequest for the next request in a redirect chain, if there is one.\n",
      " |  \n",
      " |  ok\n",
      " |      Returns True if :attr:`status_code` is less than 400, False if not.\n",
      " |      \n",
      " |      This attribute checks if the status code of the response is between\n",
      " |      400 and 600 to see if there was a client error or a server error. If\n",
      " |      the status code is between 200 and 400, this will return True. This\n",
      " |      is **not** a check to see if the response code is ``200 OK``.\n",
      " |  \n",
      " |  text\n",
      " |      Content of the response, in unicode.\n",
      " |      \n",
      " |      If Response.encoding is None, encoding will be guessed using\n",
      " |      ``chardet``.\n",
      " |      \n",
      " |      The encoding of the response content is determined based solely on HTTP\n",
      " |      headers, following RFC 2616 to the letter. If you can take advantage of\n",
      " |      non-HTTP knowledge to make a better guess at the encoding, you should\n",
      " |      set ``r.encoding`` appropriately before accessing this property.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __attrs__ = ['_content', 'status_code', 'headers', 'url', 'history', '...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- text, data descriptor : Content of the response, in unicode.\n",
    "\n",
    "- content, data descriptor : Content of the response, in bytes.\n",
    "\n",
    "- url, attribute : URL of the request\n",
    "\n",
    "- status_code, attribute : Status code returned by the server\n",
    "\n",
    "- headers, attribute : HTTP headers returned by the server\n",
    "\n",
    "- history, attribute : list of response objects holding the history of request\n",
    "\n",
    "- links, attribute : Returns the parsed header links of the response, if any.\n",
    "\n",
    "- json, method : Returns the json-encoded content of a response, if any.\n",
    "\n",
    "## Access the Response Methods and Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://crawler-test.com/'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Content-Encoding': 'gzip', 'Content-Type': 'text/html;charset=utf-8', 'Date': 'Wed, 06 Oct 2021 00:14:21 GMT', 'Server': 'nginx/1.10.3', 'Vary': 'Accept-Encoding', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-XSS-Protection': '1; mode=block', 'Content-Length': '8099', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check redirections\n",
    "url = 'https://crawler-test.com/redirects/redirect_chain_allowed'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://crawler-test.com/redirects/redirect_chain_allowed 301\n",
      "https://crawler-test.com/redirects/redirect_chain_disallowed 301\n",
      "https://crawler-test.com/redirects/redirect_target 200\n"
     ]
    }
   ],
   "source": [
    "for redirect in r.history:\n",
    "    print(redirect.url, redirect.status_code)\n",
    "print(r.url, r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://crawler-test.com/redirects/redirect_chain_allowed 301\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url, allow_redirects=False)\n",
    "print(r.url, r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'jcchouinard.com',\n",
       " 'archived_snapshots': {'closest': {'status': '200',\n",
       "   'available': True,\n",
       "   'url': 'http://web.archive.org/web/20211005023018/https://www.jcchouinard.com/',\n",
       "   'timestamp': '20211005023018'}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://archive.org/wayback/available?url=jcchouinard.com'\n",
    "r = requests.get(url)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the HTML of a page with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Crawler Test Site</title>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://crawler-test.com/'\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "soup.find('title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<meta content=\"Default description XIbwNE7SSUJciq0/Jyty\" name=\"description\"/>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('meta', attrs={'name':'description'})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Make the request\n",
    "url = 'https://crawler-test.com/'\n",
    "r = requests.get(url)\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "title = soup.find('title')\n",
    "h1 = soup.find('h1')\n",
    "description = soup.find('meta', attrs={'name':'description'})\n",
    "meta_robots = soup.find('meta', attrs={'name':'robots'})\n",
    "canonical = soup.find('link', {'rel': 'canonical'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Crawler Test Site</title>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Crawler Test Site\n",
      "h1:  Crawler Test Site\n",
      "description:  Default description XIbwNE7SSUJciq0/Jyty\n",
      "meta_robots:  \n",
      "canonical:  \n"
     ]
    }
   ],
   "source": [
    "title = title.get_text() if title else ''\n",
    "h1 = h1.get_text() if h1 else ''\n",
    "description = description['content'] if description else ''\n",
    "canonical = canonical['href'] if canonical else ''\n",
    "meta_robots = meta_robots['content'] if meta_robots else ''\n",
    "\n",
    "# Print the tags\n",
    "print('Title: ', title)\n",
    "print('h1: ', h1)\n",
    "print('description: ', description)\n",
    "print('meta_robots: ', meta_robots)\n",
    "print('canonical: ', canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all links on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://crawler-test.com/',\n",
       " 'https://crawler-test.com/mobile/separate_desktop',\n",
       " 'https://crawler-test.com/mobile/desktop_with_AMP_as_mobile',\n",
       " 'https://crawler-test.com/mobile/separate_desktop_with_different_h1',\n",
       " 'https://crawler-test.com/mobile/separate_desktop_with_different_title',\n",
       " 'https://crawler-test.com/mobile/separate_desktop_with_different_wordcount',\n",
       " 'https://crawler-test.com/mobile/separate_desktop_with_different_links_in',\n",
       " 'https://crawler-test.com/mobile/separate_desktop_with_different_links_out',\n",
       " 'https://crawler-test.com/mobile/separate_desktop_with_mobile_not_subdomain',\n",
       " 'https://crawler-test.com/mobile/desktop_with_self_canonical_mobile_and_amp']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url = 'https://crawler-test.com/'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "links = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    links.append(urljoin(url, link['href']))\n",
    "\n",
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid URL 'bad url': No schema supplied. Perhaps you meant http://bad url?\n",
      "this\n"
     ]
    }
   ],
   "source": [
    "url = 'bad url'\n",
    "\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change user-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'Listing',\n",
       " 'data': {'after': 't3_q1z7q9',\n",
       "  'dist': 1,\n",
       "  'modhash': '',\n",
       "  'geo_filter': '',\n",
       "  'children': [{'kind': 't3',\n",
       "    'data': {'approved_at_utc': None,\n",
       "     'subreddit': 'Python',\n",
       "     'selftext': \"I have loved Pycharm ever since I first started to use it, it's hard for me to let go, but....\\n\\nAt my work I have successfully transitioned from an intern to a salaried worker for a small non-profit. I have been given some projects and one of which I think is appropriate for writing Python code (doing excel automation). \\n\\nAnyhow, given how small these projects are and how infrequently I may or may not use Python/code, I cannot afford to purchase a business license for Pycharm. Does anyone have recommendations for a very similar IDE that would be free for business use?\",\n",
       "     'author_fullname': 't2_ae7gi6fr',\n",
       "     'saved': False,\n",
       "     'mod_reason_title': None,\n",
       "     'gilded': 0,\n",
       "     'clicked': False,\n",
       "     'title': 'IDE Similar to PyCharm for Work',\n",
       "     'link_flair_richtext': [{'e': 'text', 't': 'Discussion'}],\n",
       "     'subreddit_name_prefixed': 'r/Python',\n",
       "     'hidden': False,\n",
       "     'pwls': 6,\n",
       "     'link_flair_css_class': 'discussion',\n",
       "     'downs': 0,\n",
       "     'thumbnail_height': None,\n",
       "     'top_awarded_type': None,\n",
       "     'hide_score': False,\n",
       "     'name': 't3_q1z7q9',\n",
       "     'quarantine': False,\n",
       "     'link_flair_text_color': 'dark',\n",
       "     'upvote_ratio': 0.99,\n",
       "     'author_flair_background_color': None,\n",
       "     'subreddit_type': 'public',\n",
       "     'ups': 356,\n",
       "     'total_awards_received': 0,\n",
       "     'media_embed': {},\n",
       "     'thumbnail_width': None,\n",
       "     'author_flair_template_id': None,\n",
       "     'is_original_content': False,\n",
       "     'user_reports': [],\n",
       "     'secure_media': None,\n",
       "     'is_reddit_media_domain': False,\n",
       "     'is_meta': False,\n",
       "     'category': None,\n",
       "     'secure_media_embed': {},\n",
       "     'link_flair_text': 'Discussion',\n",
       "     'can_mod_post': False,\n",
       "     'score': 356,\n",
       "     'approved_by': None,\n",
       "     'is_created_from_ads_ui': False,\n",
       "     'author_premium': False,\n",
       "     'thumbnail': 'self',\n",
       "     'edited': False,\n",
       "     'author_flair_css_class': None,\n",
       "     'author_flair_richtext': [],\n",
       "     'gildings': {},\n",
       "     'content_categories': None,\n",
       "     'is_self': True,\n",
       "     'mod_note': None,\n",
       "     'created': 1633448924.0,\n",
       "     'link_flair_type': 'richtext',\n",
       "     'wls': 6,\n",
       "     'removed_by_category': None,\n",
       "     'banned_by': None,\n",
       "     'author_flair_type': 'text',\n",
       "     'domain': 'self.Python',\n",
       "     'allow_live_comments': False,\n",
       "     'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have loved Pycharm ever since I first started to use it, it&amp;#39;s hard for me to let go, but....&lt;/p&gt;\\n\\n&lt;p&gt;At my work I have successfully transitioned from an intern to a salaried worker for a small non-profit. I have been given some projects and one of which I think is appropriate for writing Python code (doing excel automation). &lt;/p&gt;\\n\\n&lt;p&gt;Anyhow, given how small these projects are and how infrequently I may or may not use Python/code, I cannot afford to purchase a business license for Pycharm. Does anyone have recommendations for a very similar IDE that would be free for business use?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;',\n",
       "     'likes': None,\n",
       "     'suggested_sort': None,\n",
       "     'banned_at_utc': None,\n",
       "     'view_count': None,\n",
       "     'archived': False,\n",
       "     'no_follow': False,\n",
       "     'is_crosspostable': False,\n",
       "     'pinned': False,\n",
       "     'over_18': False,\n",
       "     'all_awardings': [],\n",
       "     'awarders': [],\n",
       "     'media_only': False,\n",
       "     'link_flair_template_id': '0df42996-1c5e-11ea-b1a0-0e44e1c5b731',\n",
       "     'can_gild': False,\n",
       "     'spoiler': False,\n",
       "     'locked': False,\n",
       "     'author_flair_text': None,\n",
       "     'treatment_tags': [],\n",
       "     'visited': False,\n",
       "     'removed_by': None,\n",
       "     'num_reports': None,\n",
       "     'distinguished': None,\n",
       "     'subreddit_id': 't5_2qh0y',\n",
       "     'author_is_blocked': False,\n",
       "     'mod_reason_by': None,\n",
       "     'removal_reason': None,\n",
       "     'link_flair_background_color': '',\n",
       "     'id': 'q1z7q9',\n",
       "     'is_robot_indexable': True,\n",
       "     'report_reasons': None,\n",
       "     'author': 'nowimindata',\n",
       "     'discussion_type': None,\n",
       "     'num_comments': 40,\n",
       "     'send_replies': True,\n",
       "     'whitelist_status': 'all_ads',\n",
       "     'contest_mode': False,\n",
       "     'mod_reports': [],\n",
       "     'author_patreon_flair': False,\n",
       "     'author_flair_text_color': None,\n",
       "     'permalink': '/r/Python/comments/q1z7q9/ide_similar_to_pycharm_for_work/',\n",
       "     'parent_whitelist_status': 'all_ads',\n",
       "     'stickied': False,\n",
       "     'url': 'https://www.reddit.com/r/Python/comments/q1z7q9/ide_similar_to_pycharm_for_work/',\n",
       "     'subreddit_subscribers': 862438,\n",
       "     'created_utc': 1633448924.0,\n",
       "     'num_crossposts': 0,\n",
       "     'media': None,\n",
       "     'is_video': False}}],\n",
       "  'before': None}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "# https://www.whatismybrowser.com/detect/what-is-my-user-agent\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'\n",
    "\n",
    "url = 'https://www.reddit.com/r/python/top.json?limit=1&t=day'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': user_agent\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Timeout to a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPConnectionPool(host='httpbin.org', port=80): Max retries exceeded with url: /basic-auth/user/pass (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f96dc699950>, 'Connection to httpbin.org timed out. (connect timeout=0.1)'))\n"
     ]
    }
   ],
   "source": [
    "url = 'http://httpbin.org/basic-auth/user/pass'\n",
    "\n",
    "try:\n",
    "    requests.get(url, timeout=0.1)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://crawler-test.com/'\n",
    "\n",
    "proxies = {\n",
    "    'http': '128.199.237.57:8080'\n",
    "}\n",
    "\n",
    "requests.get(url, proxies=proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Headers to request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate',\n",
       "  'Authorization': 'Bearer {access_token}',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.24.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-615cee98-2dbce24d25b6481d399ff5fd'}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://httpbin.org/headers'\n",
    "\n",
    "access_token = {\n",
    "    'Authorization': 'Bearer {access_token}'\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=access_token)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests Session\n",
    "\n",
    "The session object is useful when yyou need to make requests with parameters that are persist through all the requests in a single session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1:  Bearer {access_token}\n",
      "r2:  Bearer {access_token}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "url = 'https://httpbin.org/headers'\n",
    "\n",
    "access_token = {\n",
    "    'Authorization': 'Bearer {access_token}'\n",
    "    }\n",
    "\n",
    "session.headers.update(access_token)\n",
    "\n",
    "r1 = session.get(url)\n",
    "r2 = session.get(url)\n",
    "\n",
    "print('r1: ', r1.json()['headers']['Authorization'])\n",
    "print('r2: ', r2.json()['headers']['Authorization'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f24c6761ef47301f19af8a4aa4352126c9890ab2b1972229c460a61cb7a8df0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}